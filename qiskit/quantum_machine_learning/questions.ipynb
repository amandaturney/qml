{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions/Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterized Quantum Circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameterized Quantum Circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponentially vanishing gradients (barren plateau) - The last suggestion makes sense and its intersting to think of it in comparison to neural networks and building/training those one layer at a time. In NN, the early layers are good at identifying features (such as contours, lines, etc in images) whereas the later layers use those higher-dimensional features to get an answer. So if quantum model circuits are sort of similar, I wonder how this training procedure will perform if you train it with the expectation that 1 layer should be optimized for the solution, then fix those parameters and add in another layer. Would this eliminate the possibility that the quantum circuit can work out the \"features\" of the inputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two models (manual vs qiskit) that were built and trained on the same dataset performed comparablely (90% vs 100% but for only 10 datapoints) but when we look at their parameters, overall they're not very similar. So is there anyway to figure out what the algorithm is \"learning\" from the data? Such as which features have a bigger impact on the final classification and which do not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Feature Maps and Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The QKE talks about how there are some kernels that we cannot even find the individual functions for because they are infinite-dimensional but we can't we always get some classical estimate using series approximations? If so, is there much of an advantage to a quantum algorithm that is equally limited by the sampling frequency much like a series approximation is limited by the series length?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Generative Adversarial Networks (in-progress)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
