{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Parameterized Quantum Circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are notes from each section of the qiskit course documentation found on this page: https://learn.qiskit.org/course/machine-learning/training-quantum-circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of training a quantum circuit model is to minimize a cost/loss function (objective function) with respect to the parameter vector $\\vec{\\theta}$. This equates to minimizing the expectation value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is when we have access to the gradient of our cost function and we use it to evaluate the direction of steepest descent (which is what we want for minimizing the cost function) to then adjust our current parameter values. We do this until we converge to a local minimum. We can show how to do that in Qiskit.\n",
    "\n",
    "First, we need to define our parameterized state as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 370.906x200.667 with 1 Axes>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qiskit.circuit.library import RealAmplitudes\n",
    "ansatz = RealAmplitudes(num_qubits=2, reps=1,\n",
    "                        entanglement='linear').decompose()\n",
    "ansatz.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a Hamiltonian (cost function) for which we are interested in minimizing. We'll use $\\hat{H} = \\hat{Z} \\bigotimes \\hat{Z}$. So we combine our parameterized state and measure the (average) observable defined by the Hamiltonian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f6/glc4hjkn44gg3qtb6mq_88kc0000gq/T/ipykernel_41826/1302685014.py:5: DeprecationWarning: The class ``qiskit.opflow.state_fns.operator_state_fn.OperatorStateFn`` is deprecated as of qiskit-terra 0.24.0. It will be removed no earlier than 3 months after the release date. For code migration guidelines, visit https://qisk.it/opflow_migration.\n",
      "  expectation = StateFn(hamiltonian, is_measurement=True) @ StateFn(ansatz)\n",
      "/var/folders/f6/glc4hjkn44gg3qtb6mq_88kc0000gq/T/ipykernel_41826/1302685014.py:5: DeprecationWarning: The class ``qiskit.opflow.state_fns.circuit_state_fn.CircuitStateFn`` is deprecated as of qiskit-terra 0.24.0. It will be removed no earlier than 3 months after the release date. For code migration guidelines, visit https://qisk.it/opflow_migration.\n",
      "  expectation = StateFn(hamiltonian, is_measurement=True) @ StateFn(ansatz)\n",
      "/var/folders/f6/glc4hjkn44gg3qtb6mq_88kc0000gq/T/ipykernel_41826/1302685014.py:6: DeprecationWarning: The class ``qiskit.opflow.expectations.pauli_expectation.PauliExpectation`` is deprecated as of qiskit-terra 0.24.0. It will be removed no earlier than 3 months after the release date. For code migration guidelines, visit https://qisk.it/opflow_migration.\n",
      "  pauli_basis = PauliExpectation().convert(expectation)\n"
     ]
    }
   ],
   "source": [
    "from qiskit.opflow import StateFn, PauliExpectation\n",
    "from qiskit.opflow import Z, I\n",
    "\n",
    "hamiltonian = Z ^ Z\n",
    "expectation = StateFn(hamiltonian, is_measurement=True) @ StateFn(ansatz)\n",
    "pauli_basis = PauliExpectation().convert(expectation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will evaluate the expectation value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f6/glc4hjkn44gg3qtb6mq_88kc0000gq/T/ipykernel_41826/2366752076.py:6: DeprecationWarning: The class ``qiskit.utils.quantum_instance.QuantumInstance`` is deprecated as of qiskit-terra 0.24.0. It will be removed no earlier than 3 months after the release date. For code migration guidelines, visit https://qisk.it/qi_migration.\n",
      "  quantum_instance = QuantumInstance(Aer.get_backend('qasm_simulator'),\n",
      "/var/folders/f6/glc4hjkn44gg3qtb6mq_88kc0000gq/T/ipykernel_41826/2366752076.py:10: DeprecationWarning: The class ``qiskit.opflow.converters.circuit_sampler.CircuitSampler`` is deprecated as of qiskit-terra 0.24.0. It will be removed no earlier than 3 months after the release date. For code migration guidelines, visit https://qisk.it/opflow_migration.\n",
      "  sampler = CircuitSampler(quantum_instance)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from qiskit import Aer\n",
    "from qiskit.utils import QuantumInstance\n",
    "from qiskit.opflow import PauliExpectation, CircuitSampler\n",
    "\n",
    "quantum_instance = QuantumInstance(Aer.get_backend('qasm_simulator'),\n",
    "                                   # we'll set a seed for reproducibility\n",
    "                                   shots = 8192, seed_simulator = 2718,\n",
    "                                   seed_transpiler = 2718)\n",
    "sampler = CircuitSampler(quantum_instance)\n",
    "\n",
    "def evaluate_expectation(theta):\n",
    "    value_dict = dict(zip(ansatz.parameters, theta))\n",
    "    result = sampler.convert(pauli_basis, params=value_dict).eval()\n",
    "    return np.real(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finite difference gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the simplest ways to approximate gradients is the finite difference scheme which works independently of a function's inner (possibly very complex) structure. This is basically the definition of the derivative where we evaluate the function just a tiny distance away from our current parameter value and then take the average.\n",
    "\n",
    "$gradient of f(\\vec{\\theta}) = \\frac{1}{2\\epsilon}(f(\\vec{\\theta}) + \\epsilon - f(\\vec{\\theta}) - \\epsilon)$\n",
    "\n",
    "This can be done using Qiskit's Gradient class. The finite different gradient can be volatile and noisy compared to the exact formula for the gradient and sometimes the noise can cause it to point in the opposite direction of the true gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytic gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analytics gradients evaluate the analytic formula for the gradient. While this can be fairly difficult, there is research supporting an easy formula to do this called the parameter shift rule. For a simple circuit containing of only Pauli rotations (without any coefficients), this rule says:\n",
    "\n",
    "$\\frac{\\delta f}{\\delta \\theta_i} = \\frac{f(\\vec{\\theta} + \\frac{\\pi}{2}\\vec{e_i}) - f(\\vec{\\theta} + \\frac{\\pi}{2}\\vec{e_i})}{2}$\n",
    "\n",
    "This can be done using Qiskit's Gradient class where this is done by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients use the Euclidean distance between successive parameter values whereas natural gradients use fidelity distance ($d = || \\bra{\\psi(\\vec{\\theta_n})}\\ket{\\psi(\\vec{\\theta_{n+1}})}||^2$). This is preferred because it takes into account the sensitivity of the model in regards to each parameter versus assuming the same update size for each parameter. There's a formula called the Quantum Fisher Information that is a component of the Quantum Natural Gradient (exact details can be found on the qiskit documentation webpage) that allows us to transform the steepest descent in the Euclidean parameter space to the steepest descent in the model space.\n",
    "\n",
    "![image qfi](./qfi.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The natural gradient can be computed using Qiskit's NaturalGradient class and it produces different gradient values compared to the Gradient class. In general, the natural gradient helps convergence get reached faster but at the cost of needing to evaluate many more quantum circuits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simultaneous Perturbation Stochastic Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our function $f(\\vec{\\theta})$, evaluating the gradient means calculating the partial derivative to each parameter which would be 2N function evaluations for N parameters (using shift rule or finite difference scheme where 2 evaluations are needed for each parameter).\n",
    "\n",
    "The SPSA is an optimization technique that randomly samples the gradient to reduce the number of evaluations. This will cause some jumping around but will converge (basically following the gradient descent curve) but with a fraction of the cost. This same idea can also be applied to the natural gradient as well where we sample from the quantum Fisher information.\n",
    "\n",
    "The gradient descent and natural gradient have linear and quadratic costs in terms of the number of parameters but the SPSA and QNSPSA are constant which makes them efficient algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, circuit evaluations are expensive and thus people often resort to using SPSA. To further improve convergence, the learning rate is optimized as well. Instead of using a constant learning rate, an exponentially decreasing one does a better job at making progress very quickly and then fine-tuning the updates when near convergence. This works well if you know what your loss function looks like. Qiskit will automatically calibrate the learning rate if one is not specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the above techniques all work pretty well for a _small_ model where the Qiskit documentation was working with a quantum circuit model that only had 4 parameters. We can explore how well this works for larger quantum model circuits by looking at the variance of gradients; if the variance is small, we don't have enough information to update the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponentially vanishing gradients (barren plateaus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a general parameterized quantum circuit, we can look at the relationship between the circuit size (number of qubits) and the variance of a sample of the evaluations of the (analytics) gradients at a bunch of random initial points. Overall, we see that the variance shrinks exponentially with the circuit size so the larger the circuit, the flatter our loss landscape looks which makes it hard for us to know which direction to go to find the minimum. This problem is known as the \"barren plateau\" or \"exponentially vanishing gradients\" challenge.\n",
    "\n",
    "We can also test this trend for a less deep circuit, a parameterized quantum circuit that only has 1 repeat for the rotation/entanglement layers. However, we see the exact same trend line where the variance shrinks exponentially.\n",
    "\n",
    "Another thing tested is global operators versus local operators. Global operators act on the entire circuit (such as a Z gate applied to every qubit) whereas local operators act on only a few qubits (such as a Z gate applied to the first 2 qubits and everything else gets the identity operator). This also still gives us barren plateaus.\n",
    "\n",
    "Lastly, we can try all things at once so a short depth circuit (1 layer) with local operators and we see that the variance no longer shrinks exponentially. However, these circuits seem comparable to a _small_ model circuit becuase of the local operators and they are simple due to the short depth, which makes them easy to simulate classically so they don't provide any advantage over classical models.\n",
    "\n",
    "The last modification is to train a circuit one layer at a time; so start with a single layer of rotations using local operators and optimize for the best set of parameters and then make those fixed. Then, add another layer and optimize and fix those. Continue doing this for however many layers. Testing this does show that this does potentially avoid the barren plateaus problem.\n",
    "\n",
    "<font color=\"red\">Question/Comment: </font>The last suggestion makes sense and its intersting to think of it in comparison to neural networks and building/training those one layer at a time. In NN, the early layers are good at identifying features (such as contours, lines, etc in images) whereas the later layers use those higher-dimensional features to get an answer. So if quantum model circuits are sort of similar, I wonder how this training procedure will perform if you train it with the expectation that 1 layer should be optimized for the solution, then fix those parameters and add in another layer. Would this eliminate the possibility that the quantum circuit can work out the \"features\" of the inputs?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
