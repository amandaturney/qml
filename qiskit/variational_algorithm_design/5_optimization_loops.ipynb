{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Below are notes from each section of the qiskit course documentation found on this page: https://learn.qiskit.org/course/algorithm-design/optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local and Global Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* start with an initial point of $\\theta$\n",
    "* moves to different points based on what they observe in the current region\n",
    "* iteratively search through points to find one that minimizes cost function\n",
    "* converges fast usually, but heavily dependent on initial point\n",
    "* cannot see beyond the local region and thus vulnerable to local minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* searches over several regions within the domain (non-local)\n",
    "* evaluating iteratively over a set of parameter vectors\n",
    "* slower to converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping Optimizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sets the initial value for parameters of $\\vec{\\theta}$ based on prior optimization\n",
    "* helps for faster convergence\n",
    "* $\\ket{\\psi(\\vec{\\theta_0})} = U_V(\\vec{\\theta_0})\\ket{\\rho}$\n",
    "* differs from reference state $\\ket{\\rho}$ because this is the parameters of our variational form, NOT the initial reference state which is fixed\n",
    "\n",
    "The combination of a local and global optimizer is helpful so that the non-optimal local minima are not converged on. This requires setting up two variational workloads but has more quality results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-Based and Gradient-Free Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our given cost function, if we have access to the gradient function, then we can solve for the gradient (direction of steepest descent) for our initial point and move towards that direction in a tiny step by updating our initial point to a new value in that direction. Then we evaluate the cost function again, check how much our new solution has performed, and repeat the process again (calculate gradient and move towards minima) until the solution converges on the local minima. The size of the steps we adjust our set of parameters is based on a hyperparameter call the learning rate.\n",
    "\n",
    "The main disadvantages of this type of optimization is the convergence speed (slow) and no guarantee to achieve the optimal solution (stuck in local minima)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient-Free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient-free optimization algorithms are useful when computing the gradient is difficult, expensive, or noisy. They are also more robust in finding global optima, but they require higher computational resources, especially for high-dimensional search spaces. One instance is the COBYLA optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Barren Plateaus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of the type of optimization method used, when the cost landscape is relatively flat, it is challenging for the method to determine the appropriate direction to search. This problem is known as the barren plateau where the cost landscape becomes progressively flatter and thus more challenging to determine the direction to the minimum.\n",
    "\n",
    "For a broad range of parameterized quantum circuits, the probability that the gradient along any reasonable direction is non-zero to some fixed precision decreases exponentially as the number of qubits increases.\n",
    "\n",
    "Overall, some helpful tips to boost performance are:\n",
    "\n",
    "* bootstrapping\n",
    "* experimenting with hardware-efficient ansatz to reduce noise\n",
    "* experimenting with error supression and error mitigation\n",
    "* experimenting with gradient-free optimizers which are less likely to be affected by the barren plateau"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
